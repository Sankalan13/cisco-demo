Here are the list of prompts used when setting up the project after git was initialised and the microservice demo was cloned as per instructions. NOTE: The google microservices repository was analysed manually and then a lot of discussions with various LLMs were done to come up with the execution plan and the test strategy before execution began in Claude Code.
NOTE: The plan mode and execution mode were used effectively, plan was reviewed and edited as per requirements if any deviations were found. 

- Analyse the repository and devise a plan that best utilises the existing infrastructure setup and allows me to run the microservices stack in a local kind cluster. Use shell scripts to scaffold the project deployment, create a common script called run_all.sh that orchestrates deployments. Other things will be added to this file later on. Lets review the plan and deploy the stack manually to verify if everything is running. Take a few things into account: ensure that dependency tools are installed like docker, kubectl, kind etc before continuing, if not installed, prompt the user to install these tools, check if an existing cluster is present and prompt the user if they want to delete it, do the same with the deployments. 

- Write a script that allows me to port-forward all these services so that I can access them in localhost, add the port forward script call to the run_all script so that the ports are forwarded after deployments. Ensure that the port forward happens only after the deployment replicas are set to 1/1. Also ensure that you use netcat to verify if the port connection was established after port forwarding. Port forwarding should exit if the script is terminated with command + c

- Our goal is to add a system test framework, lets start small, first we want to setup a python project, we want to use python behave so that we can document our test scenarios with ease and share the same to product facing side, we would need a separate folder in root called test-framework that will hold the tests. Add a behave.ini file so that the configuration can be managed globally. Since our serivces are using grpc calls, lets generate the protos from the protos directory in the microservices demo folder and use that to scaffold our base grpc client and all the other serivce clients. Ensure that you are creating the clients separately under the utils folder, the features folder should hold the feature files and the step definitions. This is just a project scaffolding, we will not be writing any tests in this step. Provide a plan to implement the requirement. Follow best coding practices and ensure test suite redability and scalability. 

- Now that our test scaffolding is in place, lets write our first feature file, lets pick a simple scenario where the user is browsing the product catalogue. Lets write the step definitions and ensure that the test assertions are in place. We will then run the test manually and ensure correctness. 

##The tests were run manually and verified that the response was generated and it was passing

- Since our first test is passing, we can go ahead with the rest of the infrasturcture setup. We need to first setup our infrastructure for trace collection. Our services under test have OpenCensus enabled, we need to add OpenTelemetry support with OpenTelemetryCollector and Jager so that we can view the traces. We need to then query Jager APIs to generate the traces and add it into a coverage json file that defines the user's behavioural coverage. The coverage report should show the name of the services, the endpoints on the services that were called, total number of endpoints in the service and the test covered endpoints. The number of times the API was called during the tests. 

##The to and fro for this implementation went on for a while since the two node services had a bug with OpenTelemetry Support, the model was able to find the issue after some human intervention and digging around the open telemetry support documents and github issues. Once the solution was implemented, we could see that the singe test is covering 5 API calls across 3 services. 

- Now we need to add our go lang coverage report generation. Since golang supports an out of the box profiler which generates a cover profile of the service, I want you to analyse how we can add the coverprofile generation to our existing golang services. We need to generate the coverage report after our tests are run. The coverage dumps from the services will then be pulled locally and verified. Devise a plan to implement and generate go coverage collection for our current golang services.

##The LLM went on a different tangent here, explaining how it needs to terminate the services before generating coverage and profiles, after a bit of manual search I was able to find live coverage dump using a SIGUSR1 signal. That code was added into a shared go mod and was used by all golang services. After the prompt intervention, the LLM was able to implement this properly, it was tested and verified. The coverage baseline was set at both line and behavioural level

- We need to now work on our test deployments. Till now our tests are running from localhost, but we need to run this on the CI and thus deploy it to our current services and observability stack. Lets start with writing the dockerfile for our tests and building the image. Then we will need to add the kubernetes manifests for our tests, lets add it in as a job so that the status of the job can be tracked. Lets also add a pvc to ensure that the reports are added into a storage which is extracted to localhost after the tests end. Ensure that you are adding the new deployment method into run_all.sh script such that the deployment can be switched from local to kube using a --deploy-tests flag with the script. When the tests are deployed in kubernetes, we don't need to portforward services to localhost so ensure that you add a separate services.yaml config file that uses the services names as hostnames and not localhost. Come up with a plan for this implementation and lets move ahead after I review it. 

##Dockerfile and the kube scripts were reviewed and deployed, the test run and coverage generation was also verified. There was a bit of to and for with the LLM to get everything in place. I am skipping those prompts since it was mostly questioning the decisions made by the LLM and fixing bugs and debugging. If the entire conversation log is preffered, please reach out and I shall export and provide the conversation logs. 

- Now that we have all the required infrastructure in place, we can go ahead and improve our coverage and verify the coverage increase in realtime. Currently we only have covered a single test scenario which is product catalog view path. Why don't you analyse the microservices demo repository source and come up with behave driven test scenarios focused on the user's lifecycle in the system. Lets separate out the plan into multiple phases, the first phase we focus on the high criticality scenarios which involves user playing for the product and shipping etc. Second phase we work on the lower priority scenarios and on the third phase we work on error scenarios and edge cases. Share the plan so that we can review it and move forward with implementation.

##The plan shared was executed and after a bunch of to and fro, phase 1 and 2 was completed, tests were executed and coverage improvement was verified. A bit of issues were faced when generating the jager traces since the test execution window was too small so a 30 second buffer was added on both start time and end time to generate the reports correctly. On phase 3 implementation, quickly realised that the backend APIs did not have proper input validations, negative values were also passing in the tests. This was also a reason why a lot of negative cases were not properly covered. After phase 3, we were still not covering our 9 backend services (cart service did not have opentelemetry support so we did not get any traces from there) so a phase 4 implementation was added to cover the currency service and a few other services separately. After that we were able to reach a 100% API coverage with a detailed report generated at test-framework/reports/coverage.json

- Convert all our existing shell scripts orchestration into makefiles and provide a plan to smoothly carry out the migration without breaking any existing functionality. Ensure that the support is added in for both local runs and cluster runs. Ensure to provide a make help section with proper command information, make sure that each step of the orchestration is separated out into different make commands and a single command is provided to trigger the entire orchestration. Ensure that the logs how the final test report, golang service coverage report and the trace coverage reports at the end of the test run. 

##A lot of to and for getting everything working as it was with the shell scripts, mostly the issue was in generating the logs with gocoverage summary report since the pvc report extraction was failing. I had to fix it myself after the LLM was not able to fix it after a bunch of tries. Just ensured that the report generation happens before cleanup is called and the report summary is generated locally after the reports are extracted instead of generating the report summary in the test pod itself. 

##Futher changes that I could not cover because of the weekly paid Claude pro cap was moving from Kustomise to Helm charts and helmfile for deployments. Kustomise was used since a lot of kubernetes components were changed, the LLM was applying a patch after deploying the kubernetes manifests. To remove this issue, kustomize was used, since migrating to helm and helmfile would not have taken a lot of time. Adding unit tests to all the services was left out along with UI tests due to lack of a paid plan to Claude. A bunch of improvements that can be later made to this are listed down in the readme file

Feedback:

I had a good time working on this. Here are a few things that can be done better in my opinion. It would have made my life easier and I hope it helps someone else.

- Ensuring that the candidates have paid LLM access before handing the assignment would be very helpful for future candidates. If the candidates do not have access, assigning a temporary license to the candidate would level out the field for all candidates and also allow the company to control which LLM agent is used. Completing a requirement of this scale using only free LLM services is not a valid expectation. 
- Mentioning in the test assignment if the cadidate is allowed to edit the source code to meet their needs is also an important thing missing in the assignment. I tried my best to keep the changes to the minimum but was very hesitant since a lot of places might not be expecting you to edit the source code. 
